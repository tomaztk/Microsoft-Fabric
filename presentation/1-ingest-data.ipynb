{"cells":[{"cell_type":"markdown","source":["# Part 1: Ingest data into a Microsoft Fabric lakehouse using Apache Spark\n","\n","In this tutorial, you'll ingest data into Fabric lakehouses in delta lake format. Some important terms to understand:\n","\n","* **Lakehouse** -- A lakehouse is a collection of files/folders/tables that represent a database over a data lake used by the Spark engine and SQL engine for big data processing and that includes enhanced capabilities for ACID transactions when using the open-source Delta formatted tables.\n","\n","* **Delta Lake** - Delta Lake is an open-source storage layer that brings ACID transactions, scalable metadata management, and batch and streaming data processing to Apache Spark. A Delta Lake table is a data table format that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata management."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"652c150e-e033-49f6-aacc-b1e3a807711d"},{"cell_type":"markdown","source":[],"metadata":{},"id":"ed1c494e"},{"cell_type":"markdown","source":["## Bank churn data"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"faa52bd2-a2ca-4cc4-90c2-e37b69df136d"},{"cell_type":"markdown","source":["\n","The dataset contains churn status of 10,000 customers. It also includes attributes that could impact churn such as:\n","\n","* Credit score\n","* Geographical location (Germany, France, Spain)\n","* Gender (male, female)\n","* Age\n","* Tenure (years of being bank's customer)\n","* Account balance\n","* Estimated salary\n","* Number of products that a customer has purchased through the bank\n","* Credit card status (whether a customer has a credit card or not)\n","* Active member status (whether an active bank's customer or not)\n","\n","The dataset also includes columns such as row number, customer ID, and customer surname that should have no impact on customer's decision to leave the bank. \n","\n","The event that defines the customer's churn is the closing of the customer's bank account. The column `exited` in the dataset refers to customer's abandonment. There isn't much context available about these attributes so you have to proceed without having background information about the dataset. The aim is to understand how these attributes contribute to the `exited` status.\n","\n","Example rows from the dataset:\n","\n","|\"CustomerID\"|\"Surname\"|\"CreditScore\"|\"Geography\"|\"Gender\"|\"Age\"|\"Tenure\"|\"Balance\"|\"NumOfProducts\"|\"HasCrCard\"|\"IsActiveMember\"|\"EstimatedSalary\"|\"Exited\"|\n","|---|---|---|---|---|---|---|---|---|---|---|---|---|\n","|15634602|Hargrave|619|France|Female|42|2|0.00|1|1|1|101348.88|1|\n","|15647311|Hill|608|Spain|Female|41|1|83807.86|1|0|1|112542.58|0|\n","\n","This is sample dataset with no personal data used!\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"90eec3b3-1a5d-4a3a-bfbd-78b0b035f7a9"},{"cell_type":"markdown","source":["### Download dataset and upload to lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8ade11bb-c6d2-4034-8629-6225746a0e27"},{"cell_type":"code","source":["IS_CUSTOM_DATA = False  # if TRUE, dataset has to be uploaded manually\n","\n","DATA_ROOT = \"/lakehouse/default\"\n","DATA_FOLDER = \"Files/churn\"  # folder with data files\n","DATA_FILE = \"churn.csv\"  # data file name"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"5e0e2397-5dda-4d48-b814-93ccdee6f7c1","normalized_state":"finished","queued_time":"2025-02-22T10:39:59.4026972Z","session_start_time":"2025-02-22T10:39:59.4040154Z","execution_start_time":"2025-02-22T10:41:12.6039341Z","execution_finish_time":"2025-02-22T10:41:15.3327642Z","parent_msg_id":"3a1407bb-839e-4507-b7a1-79b3c519e533"},"text/plain":"StatementMeta(, 5e0e2397-5dda-4d48-b814-93ccdee6f7c1, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"ms_comment_ranges":{},"ms_comments":[],"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae3d4f32-55a2-45f0-bda6-19ac340b708c"},{"cell_type":"markdown","source":["This code downloads a publicly available version of the dataset and then stores it in a Fabric lakehouse."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"20376a28-ff40-47a7-86a0-7fb272c88028"},{"cell_type":"code","source":["import os, requests\n","if not IS_CUSTOM_DATA:\n","# Using synapse blob, this can be done in one line\n","\n","# Download demo data files into lakehouse if not exist\n","    remote_url = \"https://synapseaisolutionsa.blob.core.windows.net/public/bankcustomerchurn\"\n","    file_list = [DATA_FILE]\n","    download_path = f\"{DATA_ROOT}/{DATA_FOLDER}/raw\"\n","\n","    if not os.path.exists(\"/lakehouse/default\"):\n","        raise FileNotFoundError(\n","            \"Default lakehouse not found, please add a lakehouse and restart the session.\"\n","        )\n","    os.makedirs(download_path, exist_ok=True)\n","    for fname in file_list:\n","        if not os.path.exists(f\"{download_path}/{fname}\"):\n","            r = requests.get(f\"{remote_url}/{fname}\", timeout=30)\n","            with open(f\"{download_path}/{fname}\", \"wb\") as f:\n","                f.write(r.content)\n","    print(\"Downloaded demo data files into lakehouse.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"db4d22ce-d1b1-4381-9968-9fd9bc4b5c92","normalized_state":"finished","queued_time":"2024-10-25T11:22:38.2966149Z","session_start_time":null,"execution_start_time":"2024-10-25T11:22:38.7823899Z","execution_finish_time":"2024-10-25T11:22:39.6690932Z","parent_msg_id":"9e60059d-59f7-40ca-80c1-6c1afb878302"},"text/plain":"StatementMeta(, db4d22ce-d1b1-4381-9968-9fd9bc4b5c92, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloaded demo data files into lakehouse.\n"]}],"execution_count":2,"metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"696ee4b1-dbea-4479-af62-69135be55634"},{"cell_type":"markdown","source":["## Next step\n","\n","GoTO: \"Part 2: Explore and clean the data - EDA\""],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5cc56493-b776-4662-a0a1-ef65906c7fb2"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"notebook_environment":{},"nteract":{"version":"nteract-front-end@1.0.0"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"},"enableDebugMode":false}},"dependencies":{"lakehouse":{"default_lakehouse":"8254f032-6f1c-4f47-ad3a-f1ec5019be2f","default_lakehouse_name":"LK_flights","default_lakehouse_workspace_id":"3db7091c-9d93-489e-930b-f676a1179736"}}},"nbformat":4,"nbformat_minor":5}